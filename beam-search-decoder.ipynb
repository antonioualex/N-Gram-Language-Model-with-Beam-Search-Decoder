{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XpChAX7pbuB6",
    "outputId": "a1782121-6a2c-4103-fb4a-ea9203d8ff72",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:37:18.253451Z",
     "start_time": "2024-07-05T14:37:16.426294Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/antonal/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import Levenshtein as lev\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import reuters\n",
    "from collections import Counter\n",
    "from datasets import load_metric\n",
    "from tqdm import tqdm\n",
    "\n",
    "_ = nltk.download(\"reuters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Function to create vocabulary from text tokens\n",
    "def create_vocab(text_tokens, min_count=10):\n",
    "    # compute the count of each token and create the vocabulary from those whose count is 10 or more\n",
    "    filtered_tokens_freq = nltk.FreqDist(text_tokens)\n",
    "    created_vocabulary = [k for k, v in filtered_tokens_freq.items() if v >= min_count]\n",
    "    return created_vocabulary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-05T14:37:18.255962Z",
     "start_time": "2024-07-05T14:37:18.254074Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Get the file ids of the documents in the training subset\n",
    "training_file_ids = [\n",
    "    file_id for file_id in reuters.fileids() if file_id.startswith(\"training/\")\n",
    "]\n",
    "training_text = reuters.raw(training_file_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-05T14:37:18.542875Z",
     "start_time": "2024-07-05T14:37:18.256706Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# get the tokens for the vocabulary creation\n",
    "tokens = word_tokenize(training_text)\n",
    "\n",
    "# Create the vocabulary\n",
    "vocabulary = create_vocab(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-05T14:37:21.047570Z",
     "start_time": "2024-07-05T14:37:18.543709Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Replace the tokens in sentences with *UNK*\n",
    "sentences = sent_tokenize(training_text)\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Replace token which are not in the vocabulary with \"*UNK*\"\n",
    "processed_sentences = [\n",
    "    [token if token in vocabulary else \"*UNK*\" for token in tokenized_sentence]\n",
    "    for tokenized_sentence in tokenized_sentences\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-05T14:37:35.494741Z",
     "start_time": "2024-07-05T14:37:21.103495Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## N-gram Language Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Initialize the unigram, bigram, and trigram counters\n",
    "unigram_counter = Counter()\n",
    "bigram_counter = Counter()\n",
    "trigram_counter = Counter()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-05T14:37:35.496399Z",
     "start_time": "2024-07-05T14:37:35.495221Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Zb6os7PVsCA",
    "outputId": "92fd639d-7676-4b91-d64b-8db2d4d3e7f5",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:37:36.239326Z",
     "start_time": "2024-07-05T14:37:35.596143Z"
    }
   },
   "outputs": [],
   "source": [
    "for sent in processed_sentences:\n",
    "    # Update the unigram counter\n",
    "    unigram_counter.update([(gram,) for gram in [\"<s>\"] + sent])\n",
    "\n",
    "    # Update the bigram counter\n",
    "    bigram_pad_sent = [\"<s>\"] + sent + [\"<e>\"]\n",
    "    bigram_counter.update(\n",
    "        [(gram1, gram2) for gram1, gram2 in zip(bigram_pad_sent, bigram_pad_sent[1:])]\n",
    "    )\n",
    "\n",
    "    # Update the trigram counter\n",
    "    trigram_pad_sent = [\"<s>\"] * 2 + sent + [\"<e>\"] * 2\n",
    "    trigram_counter.update(\n",
    "        [\n",
    "            (gram1, gram2, gram3)\n",
    "            for gram1, gram2, gram3 in zip(\n",
    "                trigram_pad_sent, trigram_pad_sent[1:], trigram_pad_sent[2:]\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hLDfe__QDnUC",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:37:36.243406Z",
     "start_time": "2024-07-05T14:37:36.241Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[(('*UNK*',), 93422),\n (('the',), 43182),\n ((',',), 39586),\n (('<s>',), 37700),\n (('.',), 37651)]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The 5 most common unigrams\n",
    "unigram_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pyXh8jUyDiJA",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:37:36.252483Z",
     "start_time": "2024-07-05T14:37:36.250400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[(('.', '<e>'), 36457),\n (('*UNK*', '*UNK*'), 9364),\n (('<s>', '*UNK*'), 7194),\n (('<s>', 'The'), 6600),\n (('&', 'lt'), 6300)]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The 5 most common bigrams\n",
    "bigram_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "[(('.', '<e>', '<e>'), 36457),\n (('<s>', '<s>', '*UNK*'), 7194),\n (('<s>', '<s>', 'The'), 6600),\n (('&', 'lt', ';'), 6300),\n (('said', '.', '<e>'), 5924)]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The 5 most common trigrams\n",
    "trigram_counter.most_common(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-05T14:37:36.292503Z",
     "start_time": "2024-07-05T14:37:36.270271Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "r-7Tv6y3buCA",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:37:36.292772Z",
     "start_time": "2024-07-05T14:37:36.273066Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_bigram_prob(\n",
    "    bigram_vocabulary, bigram_counter, unigram_counter, alpha, first_word, second_word\n",
    "):\n",
    "    # Calculate vocab size\n",
    "    bigram_vocab_size = len(bigram_vocabulary)\n",
    "\n",
    "    # Bigram prob + laplace smoothing\n",
    "    bigram_prob = (bigram_counter[(first_word, second_word)] + alpha) / (\n",
    "        unigram_counter[(first_word,)] + alpha * bigram_vocab_size\n",
    "    )\n",
    "\n",
    "    # Calculate log probability\n",
    "    bigram_log_prob = math.log2(bigram_prob)\n",
    "\n",
    "    return bigram_prob, bigram_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "js-hB9HHdIba",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:37:36.292836Z",
     "start_time": "2024-07-05T14:37:36.287968Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_trigram_prob(\n",
    "    vocabulary, trigram_counter, bigram_counter, alpha, word1, word2, word3\n",
    "):\n",
    "    # Calculate vocab size\n",
    "    vocab_size = len(vocabulary)\n",
    "\n",
    "    # Bigram prob + laplace smoothing\n",
    "    trigram_prob = (trigram_counter[(word1, word2, word3)] + alpha) / (\n",
    "        bigram_counter[\n",
    "            (\n",
    "                word1,\n",
    "                word2,\n",
    "            )\n",
    "        ]\n",
    "        + alpha * vocab_size\n",
    "    )\n",
    "\n",
    "    # Calculate log probability\n",
    "    trigram_log_prob = math.log2(trigram_prob)\n",
    "\n",
    "    return trigram_prob, trigram_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YGDU6tlirg7c",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:37:36.512028Z",
     "start_time": "2024-07-05T14:37:36.288139Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the file ids of the documents in the testing subset\n",
    "testing_file_ids = [\n",
    "    file_id for file_id in reuters.fileids() if file_id.startswith(\"test/\")\n",
    "]\n",
    "testing_text = reuters.raw(testing_file_ids[:100])\n",
    "testing_sentences = sent_tokenize(testing_text)\n",
    "\n",
    "tokenized_testing_sentences = [\n",
    "    word_tokenize(sentence) for sentence in testing_sentences\n",
    "]\n",
    "\n",
    "# Replace token which are not in the vocabulary with \"*UNK*\"\n",
    "processed_testing_sentences = [\n",
    "    [token if token in vocabulary else \"*UNK*\" for token in tokenized_sentence]\n",
    "    for tokenized_sentence in tokenized_testing_sentences\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkxG3zs7siOV",
    "outputId": "cb3c4fe0-0f62-4aed-c047-42a5fb902c7a",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:37:36.528425Z",
     "start_time": "2024-07-05T14:37:36.526184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram:\n",
      "\tCross Entropy: 8.353\n",
      "\tPerplexity: 327.069\n"
     ]
    }
   ],
   "source": [
    "sum_log_prob = 0\n",
    "bigram_cnt = 0\n",
    "alpha = 1\n",
    "\n",
    "for sent in processed_testing_sentences:\n",
    "    sent = [\"<s>\"] + sent + [\"<e>\"]\n",
    "\n",
    "    # Iterate over the bigrams of the sentence\n",
    "    for idx in range(1, len(sent)):\n",
    "        bigram_prob, bigram_log_prob = calculate_bigram_prob(\n",
    "            vocabulary, bigram_counter, unigram_counter, alpha, sent[idx - 1], sent[idx]\n",
    "        )\n",
    "\n",
    "        sum_log_prob += bigram_log_prob\n",
    "        bigram_cnt += 1\n",
    "\n",
    "HC = -sum_log_prob / bigram_cnt\n",
    "perpl = math.pow(2, HC)\n",
    "print(\"Bigram:\")\n",
    "print(\"\\tCross Entropy: {0:.3f}\".format(HC))\n",
    "print(\"\\tPerplexity: {0:.3f}\".format(perpl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74nKRvu2tL62",
    "outputId": "75c63135-6e6c-4699-b25b-abe02095415b",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:37:36.549173Z",
     "start_time": "2024-07-05T14:37:36.546744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram:\n",
      "\tCross Entropy: 10.356\n",
      "\tPerplexity: 1310.347\n"
     ]
    }
   ],
   "source": [
    "sum_log_prob = 0\n",
    "trigram_cnt = 0\n",
    "alpha = 1\n",
    "\n",
    "for sent in processed_testing_sentences:\n",
    "    sent = [\"<s>\"] + [\"<s>\"] + sent + [\"<e>\"] + [\"<e>\"]\n",
    "\n",
    "    for idx in range(2, len(sent) - 1):\n",
    "        trigram_prob, trigram_log_prob = calculate_trigram_prob(\n",
    "            vocabulary,\n",
    "            trigram_counter,\n",
    "            bigram_counter,\n",
    "            alpha,\n",
    "            sent[idx - 2],\n",
    "            sent[idx - 1],\n",
    "            sent[idx],\n",
    "        )\n",
    "        sum_log_prob += trigram_log_prob\n",
    "        trigram_cnt += 1\n",
    "\n",
    "HC = -sum_log_prob / trigram_cnt\n",
    "perpl = math.pow(2, HC)\n",
    "print(\"Trigram:\")\n",
    "print(\"\\tCross Entropy: {0:.3f}\".format(HC))\n",
    "print(\"\\tPerplexity: {0:.3f}\".format(perpl))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Noisy Text Generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Bd8bEtIo5UZ8",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:37:36.552186Z",
     "start_time": "2024-07-05T14:37:36.550218Z"
    }
   },
   "outputs": [],
   "source": [
    "# method to create a new text with random errors\n",
    "def wrong_text_creator(text, error_probability=0.05):\n",
    "    result = []\n",
    "    for sentence in text:\n",
    "        wrong_sent = []\n",
    "        for char in sentence:\n",
    "            if char.isspace():\n",
    "                wrong_sent.append(char)\n",
    "            elif char.isalpha():\n",
    "                if random.random() < error_probability:\n",
    "                    wrong_sent.append(random.choice(string.ascii_letters))\n",
    "                else:\n",
    "                    wrong_sent.append(char)\n",
    "            elif char.isnumeric():\n",
    "                if random.random() < error_probability:\n",
    "                    wrong_sent.append(random.choice(string.digits))\n",
    "                else:\n",
    "                    wrong_sent.append(char)\n",
    "            elif char in string.punctuation:\n",
    "                if random.random() < error_probability:\n",
    "                    wrong_sent.append(random.choice(string.punctuation))\n",
    "                else:\n",
    "                    wrong_sent.append(char)\n",
    "            else:\n",
    "                wrong_sent.append(char)\n",
    "        result.append(\"\".join(wrong_sent))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jNaYSaITbn4o",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:37:36.637498Z",
     "start_time": "2024-07-05T14:37:36.607919Z"
    }
   },
   "outputs": [],
   "source": [
    "# Store the train bigram probabilities for beam search decoder function\n",
    "bigram_probs_dict = {}\n",
    "alpha = 1\n",
    "vocab_size = len(set(vocabulary))\n",
    "for set_of_2 in list(bigram_counter.items()):\n",
    "    bigram_prob = (set_of_2[1] + alpha) / (\n",
    "        unigram_counter[(set_of_2[0][0],)] + alpha * vocab_size\n",
    "    )\n",
    "    bigram_probs_dict[set_of_2[0]] = bigram_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Beam Search Decoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "eOIeDl7x_MRK",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:37:36.640550Z",
     "start_time": "2024-07-05T14:37:36.638666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Beam search decoder\n",
    "def beam_search_decoder(\n",
    "    input_sentence, bigram_probabilities, vocabulary, max_depth, beam_size=2\n",
    "):\n",
    "    input_tokens = [\"<s>\"] + word_tokenize(input_sentence) + [\"<e>\"]\n",
    "    lambda_1 = 0.5\n",
    "    lambda_2 = 0.5\n",
    "\n",
    "    beam = [([], 0)]\n",
    "    for current_word in input_tokens[1:]:\n",
    "        candidates = []\n",
    "        for candidate, candidate_score in beam:\n",
    "            for vocab_word in vocabulary:\n",
    "                temp_candidate = \"<s>\" if not candidate else candidate[-1]\n",
    "                bigram_prob = bigram_probabilities.get((temp_candidate, vocab_word), 0)\n",
    "                distance = lev.distance(current_word, vocab_word)\n",
    "                word_similarity = max(len(current_word), len(vocab_word)) - distance\n",
    "                new_score = (\n",
    "                    candidate_score\n",
    "                    + lambda_1 * math.log2(1 + bigram_prob)\n",
    "                    + lambda_2 * math.log2(1 + word_similarity)\n",
    "                )\n",
    "                new_candidate = candidate + [vocab_word]\n",
    "                candidates.append((new_candidate, new_score))\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        beam = candidates[:beam_size]\n",
    "        if len(beam[0][0]) >= max_depth:\n",
    "            break\n",
    "\n",
    "    return \" \".join(beam[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "FG6f_ZUwek5h",
    "outputId": "9199f427-01ed-41f6-f054-2aba63a702d9",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:48:50.180428Z",
     "start_time": "2024-07-05T14:37:36.648030Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Correcting sentences: 100%|██████████| 619/619 [11:13<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "corrected_sentences = []\n",
    "# create the faulty text\n",
    "noisy_sentences = wrong_text_creator(testing_sentences)\n",
    "\n",
    "for wrong_sentence in tqdm(noisy_sentences, desc=\"Correcting sentences\"):\n",
    "    corrected_sentence = beam_search_decoder(\n",
    "        wrong_sentence, bigram_probs_dict, vocabulary, len(wrong_sentence)\n",
    "    )\n",
    "    corrected_sentences.append(corrected_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              Original  \\\n0    ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...   \n1    They told Reuter correspondents in Asian capit...   \n2    But some exporters said that while the conflic...   \n3    The U.S. Has said it will impose 300 mln dlrs ...   \n4    Unofficial Japanese estimates put the impact o...   \n..                                                 ...   \n614  A dividend of 11 marks would\\n  be proposed fo...   \n615  Share analysts said they saw supervisory board...   \n616  \"Anything else would be more than a surprise,\"...   \n617  Company sources said VW would have to dig into...   \n618  Parent company reserves stood at\\n  around thr...   \n\n                                                 Noisy  \\\n0    ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-yAPAN RI...   \n1    They told Reuter correspondents in Asian cBQit...   \n2    But some exporters Saiw that while the cenflic...   \n3    Tce U.S. Has said it will impose 300 mln dlrO ...   \n4    USofficiEl Japanese estimates put the impaIt o...   \n..                                                 ...   \n614  A Nividend of 11 marks would\\n  be proposed fo...   \n615  Share analysts said they saw superJisory board...   \n616  \"Anything else would be more than a surpFise,\"...   \n617  lompany sources sOid VW woQld haLe to dig iVto...   \n618  Parent company reservqs stoog at\\n  around thr...   \n\n                                             Corrected  \n0    AUSTRALIAN EXPORTERS FEBRUARY DAMAGE FROM U.S....  \n1    They told Reuters corresponding in Australian ...  \n2    But some exporters Statistics that while the c...  \n3    Thatcher U.S. Has said it will impose 300 mln ...  \n4    official Japanese estimates put the impact on ...  \n..                                                 ...  \n614  A dividend of 1.1 markets would be proposed fo...  \n615  Share analysts said they saw superior board ap...  \n616  `` anything else would be more than a surprise...  \n617  Company sources said Venezuela would have to d...  \n618  Parent company reserves stood at around three ...  \n\n[619 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Original</th>\n      <th>Noisy</th>\n      <th>Corrected</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...</td>\n      <td>ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-yAPAN RI...</td>\n      <td>AUSTRALIAN EXPORTERS FEBRUARY DAMAGE FROM U.S....</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>They told Reuter correspondents in Asian capit...</td>\n      <td>They told Reuter correspondents in Asian cBQit...</td>\n      <td>They told Reuters corresponding in Australian ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>But some exporters said that while the conflic...</td>\n      <td>But some exporters Saiw that while the cenflic...</td>\n      <td>But some exporters Statistics that while the c...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The U.S. Has said it will impose 300 mln dlrs ...</td>\n      <td>Tce U.S. Has said it will impose 300 mln dlrO ...</td>\n      <td>Thatcher U.S. Has said it will impose 300 mln ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Unofficial Japanese estimates put the impact o...</td>\n      <td>USofficiEl Japanese estimates put the impaIt o...</td>\n      <td>official Japanese estimates put the impact on ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>614</th>\n      <td>A dividend of 11 marks would\\n  be proposed fo...</td>\n      <td>A Nividend of 11 marks would\\n  be proposed fo...</td>\n      <td>A dividend of 1.1 markets would be proposed fo...</td>\n    </tr>\n    <tr>\n      <th>615</th>\n      <td>Share analysts said they saw supervisory board...</td>\n      <td>Share analysts said they saw superJisory board...</td>\n      <td>Share analysts said they saw superior board ap...</td>\n    </tr>\n    <tr>\n      <th>616</th>\n      <td>\"Anything else would be more than a surprise,\"...</td>\n      <td>\"Anything else would be more than a surpFise,\"...</td>\n      <td>`` anything else would be more than a surprise...</td>\n    </tr>\n    <tr>\n      <th>617</th>\n      <td>Company sources said VW would have to dig into...</td>\n      <td>lompany sources sOid VW woQld haLe to dig iVto...</td>\n      <td>Company sources said Venezuela would have to d...</td>\n    </tr>\n    <tr>\n      <th>618</th>\n      <td>Parent company reserves stood at\\n  around thr...</td>\n      <td>Parent company reservqs stoog at\\n  around thr...</td>\n      <td>Parent company reserves stood at around three ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>619 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the original, noisy and corrected sentences into a dataframe so that we can compare them\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Original\": testing_sentences,\n",
    "        \"Noisy\": noisy_sentences,\n",
    "        \"Corrected\": corrected_sentences,\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-05T14:48:50.192682Z",
     "start_time": "2024-07-05T14:48:50.187212Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Ms2GTfxFA1bg",
    "outputId": "b68c11c0-8c75-40ad-cc90-513cb7d9f0a8",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:48:54.681588Z",
     "start_time": "2024-07-05T14:48:50.193561Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dp/w8c9sz4j6rxbq418ktj26ycr0000gn/T/ipykernel_8398/3173324535.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  cer = load_metric(\"cer\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/2.16k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0ae8769958647ec83477b4ec13232d6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Loading cer requires you to execute the dataset script in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Character error rate metric\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m cer \u001B[38;5;241m=\u001B[39m \u001B[43mload_metric\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcer\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Word error rate metric\u001B[39;00m\n\u001B[1;32m      4\u001B[0m wer \u001B[38;5;241m=\u001B[39m load_metric(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwer\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/ta2/lib/python3.11/site-packages/datasets/utils/deprecation_utils.py:46\u001B[0m, in \u001B[0;36mdeprecated.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     44\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(warning_msg, category\u001B[38;5;241m=\u001B[39m\u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m     45\u001B[0m     _emitted_deprecation_warnings\u001B[38;5;241m.\u001B[39madd(func_hash)\n\u001B[0;32m---> 46\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdeprecated_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ta2/lib/python3.11/site-packages/datasets/load.py:2111\u001B[0m, in \u001B[0;36mload_metric\u001B[0;34m(path, config_name, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, trust_remote_code, **metric_init_kwargs)\u001B[0m\n\u001B[1;32m   2108\u001B[0m warnings\u001B[38;5;241m.\u001B[39mfilterwarnings(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m, message\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.*https://huggingface.co/docs/evaluate$\u001B[39m\u001B[38;5;124m\"\u001B[39m, category\u001B[38;5;241m=\u001B[39m\u001B[38;5;167;01mFutureWarning\u001B[39;00m)\n\u001B[1;32m   2110\u001B[0m download_mode \u001B[38;5;241m=\u001B[39m DownloadMode(download_mode \u001B[38;5;129;01mor\u001B[39;00m DownloadMode\u001B[38;5;241m.\u001B[39mREUSE_DATASET_IF_EXISTS)\n\u001B[0;32m-> 2111\u001B[0m metric_module \u001B[38;5;241m=\u001B[39m \u001B[43mmetric_module_factory\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2112\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2113\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2114\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2115\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2116\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2117\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mmodule_path\n\u001B[1;32m   2118\u001B[0m metric_cls \u001B[38;5;241m=\u001B[39m import_main_class(metric_module, dataset\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   2119\u001B[0m metric \u001B[38;5;241m=\u001B[39m metric_cls(\n\u001B[1;32m   2120\u001B[0m     config_name\u001B[38;5;241m=\u001B[39mconfig_name,\n\u001B[1;32m   2121\u001B[0m     process_id\u001B[38;5;241m=\u001B[39mprocess_id,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2126\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmetric_init_kwargs,\n\u001B[1;32m   2127\u001B[0m )\n",
      "File \u001B[0;32m~/ta2/lib/python3.11/site-packages/datasets/utils/deprecation_utils.py:46\u001B[0m, in \u001B[0;36mdeprecated.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     44\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(warning_msg, category\u001B[38;5;241m=\u001B[39m\u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m     45\u001B[0m     _emitted_deprecation_warnings\u001B[38;5;241m.\u001B[39madd(func_hash)\n\u001B[0;32m---> 46\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdeprecated_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ta2/lib/python3.11/site-packages/datasets/load.py:2029\u001B[0m, in \u001B[0;36mmetric_module_factory\u001B[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, trust_remote_code, **download_kwargs)\u001B[0m\n\u001B[1;32m   2027\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:  \u001B[38;5;66;03m# noqa if it's not in the cache, then it doesn't exist.\u001B[39;00m\n\u001B[1;32m   2028\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e1, \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m):\n\u001B[0;32m-> 2029\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m e1 \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2030\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\n\u001B[1;32m   2031\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCouldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find a metric script at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrelative_to_absolute_path(combined_path)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2032\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMetric \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt exist on the Hugging Face Hub either.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2033\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/ta2/lib/python3.11/site-packages/datasets/load.py:2023\u001B[0m, in \u001B[0;36mmetric_module_factory\u001B[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, trust_remote_code, **download_kwargs)\u001B[0m\n\u001B[1;32m   2014\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_relative_path(path) \u001B[38;5;129;01mand\u001B[39;00m path\u001B[38;5;241m.\u001B[39mcount(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   2015\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   2016\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mGithubMetricModuleFactory\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2017\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2018\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2019\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2020\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2021\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdynamic_modules_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdynamic_modules_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2022\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m-> 2023\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2024\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e1:  \u001B[38;5;66;03m# noqa all the attempts failed, before raising the error we should check if the module is already cached.\u001B[39;00m\n\u001B[1;32m   2025\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/ta2/lib/python3.11/site-packages/datasets/load.py:812\u001B[0m, in \u001B[0;36mGithubMetricModuleFactory.get_module\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    801\u001B[0m         _create_importable_file(\n\u001B[1;32m    802\u001B[0m             local_path\u001B[38;5;241m=\u001B[39mlocal_path,\n\u001B[1;32m    803\u001B[0m             local_imports\u001B[38;5;241m=\u001B[39mlocal_imports,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    809\u001B[0m             download_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownload_mode,\n\u001B[1;32m    810\u001B[0m         )\n\u001B[1;32m    811\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 812\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    813\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m requires you to execute the dataset script in that\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    814\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m repo on your local machine. Make sure you have read the code there to avoid malicious use, then\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    815\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m set the option `trust_remote_code=True` to remove this error.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    816\u001B[0m         )\n\u001B[1;32m    817\u001B[0m module_path, \u001B[38;5;28mhash\u001B[39m \u001B[38;5;241m=\u001B[39m _load_importable_file(\n\u001B[1;32m    818\u001B[0m     dynamic_modules_path\u001B[38;5;241m=\u001B[39mdynamic_modules_path,\n\u001B[1;32m    819\u001B[0m     module_namespace\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetrics\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    820\u001B[0m     subdirectory_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mhash\u001B[39m,\n\u001B[1;32m    821\u001B[0m     name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname,\n\u001B[1;32m    822\u001B[0m )\n\u001B[1;32m    823\u001B[0m \u001B[38;5;66;03m# make the new module to be noticed by the import system\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: Loading cer requires you to execute the dataset script in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error."
     ]
    }
   ],
   "source": [
    "# Character error rate metric\n",
    "cer = load_metric(\"cer\")\n",
    "# Word error rate metric\n",
    "wer = load_metric(\"wer\")\n",
    "\n",
    "truth_sentences = testing_sentences\n",
    "\n",
    "# Compute the WER and CER scores\n",
    "wer_score = wer.compute(predictions=corrected_sentences, references=truth_sentences)\n",
    "cer_score = cer.compute(predictions=corrected_sentences, references=truth_sentences)\n",
    "\n",
    "print(\"WER score: {0:.3f}\".format(wer_score))\n",
    "print(\"CER score: {0:.3f}\".format(cer_score))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
