{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XpChAX7pbuB6",
    "outputId": "a1782121-6a2c-4103-fb4a-ea9203d8ff72",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:56:33.815194Z",
     "start_time": "2024-07-05T14:56:32.475750Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/antonal/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import Levenshtein as lev\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import reuters\n",
    "from collections import Counter\n",
    "from datasets import load_metric\n",
    "from tqdm import tqdm\n",
    "\n",
    "_ = nltk.download(\"reuters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Function to create vocabulary from text tokens\n",
    "def create_vocab(text_tokens, min_count=10):\n",
    "    # compute the count of each token and create the vocabulary from those whose count is 10 or more\n",
    "    filtered_tokens_freq = nltk.FreqDist(text_tokens)\n",
    "    created_vocabulary = [k for k, v in filtered_tokens_freq.items() if v >= min_count]\n",
    "    return created_vocabulary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-05T14:56:33.817988Z",
     "start_time": "2024-07-05T14:56:33.816458Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Get the file ids of the documents in the training subset\n",
    "training_file_ids = [\n",
    "    file_id for file_id in reuters.fileids() if file_id.startswith(\"training/\")\n",
    "]\n",
    "training_text = reuters.raw(training_file_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-05T14:56:34.108788Z",
     "start_time": "2024-07-05T14:56:33.819418Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# get the tokens for the vocabulary creation\n",
    "tokens = word_tokenize(training_text)\n",
    "\n",
    "# Create the vocabulary\n",
    "vocabulary = create_vocab(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-05T14:56:36.683630Z",
     "start_time": "2024-07-05T14:56:34.109858Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Replace the tokens in sentences with *UNK*\n",
    "sentences = sent_tokenize(training_text)\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Replace token which are not in the vocabulary with \"*UNK*\"\n",
    "processed_sentences = [\n",
    "    [token if token in vocabulary else \"*UNK*\" for token in tokenized_sentence]\n",
    "    for tokenized_sentence in tokenized_sentences\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-05T14:56:50.937911Z",
     "start_time": "2024-07-05T14:56:36.688817Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## N-gram Language Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Initialize the unigram, bigram, and trigram counters\n",
    "unigram_counter = Counter()\n",
    "bigram_counter = Counter()\n",
    "trigram_counter = Counter()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-05T14:56:50.939905Z",
     "start_time": "2024-07-05T14:56:50.938182Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Zb6os7PVsCA",
    "outputId": "92fd639d-7676-4b91-d64b-8db2d4d3e7f5",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:56:51.677617Z",
     "start_time": "2024-07-05T14:56:51.059470Z"
    }
   },
   "outputs": [],
   "source": [
    "for sent in processed_sentences:\n",
    "    # Update the unigram counter\n",
    "    unigram_counter.update([(gram,) for gram in [\"<s>\"] + sent])\n",
    "\n",
    "    # Update the bigram counter\n",
    "    bigram_pad_sent = [\"<s>\"] + sent + [\"<e>\"]\n",
    "    bigram_counter.update(\n",
    "        [(gram1, gram2) for gram1, gram2 in zip(bigram_pad_sent, bigram_pad_sent[1:])]\n",
    "    )\n",
    "\n",
    "    # Update the trigram counter\n",
    "    trigram_pad_sent = [\"<s>\"] * 2 + sent + [\"<e>\"] * 2\n",
    "    trigram_counter.update(\n",
    "        [\n",
    "            (gram1, gram2, gram3)\n",
    "            for gram1, gram2, gram3 in zip(\n",
    "                trigram_pad_sent, trigram_pad_sent[1:], trigram_pad_sent[2:]\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hLDfe__QDnUC",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:56:51.681241Z",
     "start_time": "2024-07-05T14:56:51.679563Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[(('*UNK*',), 93422),\n (('the',), 43182),\n ((',',), 39586),\n (('<s>',), 37700),\n (('.',), 37651)]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The 5 most common unigrams\n",
    "unigram_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pyXh8jUyDiJA",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:56:51.689848Z",
     "start_time": "2024-07-05T14:56:51.688439Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[(('.', '<e>'), 36457),\n (('*UNK*', '*UNK*'), 9364),\n (('<s>', '*UNK*'), 7194),\n (('<s>', 'The'), 6600),\n (('&', 'lt'), 6300)]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The 5 most common bigrams\n",
    "bigram_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "[(('.', '<e>', '<e>'), 36457),\n (('<s>', '<s>', '*UNK*'), 7194),\n (('<s>', '<s>', 'The'), 6600),\n (('&', 'lt', ';'), 6300),\n (('said', '.', '<e>'), 5924)]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The 5 most common trigrams\n",
    "trigram_counter.most_common(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-05T14:56:51.738139Z",
     "start_time": "2024-07-05T14:56:51.708012Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "r-7Tv6y3buCA",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:56:51.740987Z",
     "start_time": "2024-07-05T14:56:51.710560Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_bigram_prob(\n",
    "    bigram_vocabulary, bigram_counter, unigram_counter, alpha, first_word, second_word\n",
    "):\n",
    "    # Calculate vocab size\n",
    "    bigram_vocab_size = len(bigram_vocabulary)\n",
    "\n",
    "    # Bigram prob + laplace smoothing\n",
    "    bigram_prob = (bigram_counter[(first_word, second_word)] + alpha) / (\n",
    "        unigram_counter[(first_word,)] + alpha * bigram_vocab_size\n",
    "    )\n",
    "\n",
    "    # Calculate log probability\n",
    "    bigram_log_prob = math.log2(bigram_prob)\n",
    "\n",
    "    return bigram_prob, bigram_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "js-hB9HHdIba",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:56:51.741119Z",
     "start_time": "2024-07-05T14:56:51.712921Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_trigram_prob(\n",
    "    vocabulary, trigram_counter, bigram_counter, alpha, word1, word2, word3\n",
    "):\n",
    "    # Calculate vocab size\n",
    "    vocab_size = len(vocabulary)\n",
    "\n",
    "    # Bigram prob + laplace smoothing\n",
    "    trigram_prob = (trigram_counter[(word1, word2, word3)] + alpha) / (\n",
    "        bigram_counter[\n",
    "            (\n",
    "                word1,\n",
    "                word2,\n",
    "            )\n",
    "        ]\n",
    "        + alpha * vocab_size\n",
    "    )\n",
    "\n",
    "    # Calculate log probability\n",
    "    trigram_log_prob = math.log2(trigram_prob)\n",
    "\n",
    "    return trigram_prob, trigram_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YGDU6tlirg7c",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:56:51.948196Z",
     "start_time": "2024-07-05T14:56:51.725047Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the file ids of the documents in the testing subset\n",
    "testing_file_ids = [\n",
    "    file_id for file_id in reuters.fileids() if file_id.startswith(\"test/\")\n",
    "]\n",
    "testing_text = reuters.raw(testing_file_ids[:100])\n",
    "testing_sentences = sent_tokenize(testing_text)\n",
    "\n",
    "tokenized_testing_sentences = [\n",
    "    word_tokenize(sentence) for sentence in testing_sentences\n",
    "]\n",
    "\n",
    "# Replace token which are not in the vocabulary with \"*UNK*\"\n",
    "processed_testing_sentences = [\n",
    "    [token if token in vocabulary else \"*UNK*\" for token in tokenized_sentence]\n",
    "    for tokenized_sentence in tokenized_testing_sentences\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkxG3zs7siOV",
    "outputId": "cb3c4fe0-0f62-4aed-c047-42a5fb902c7a",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:56:51.962772Z",
     "start_time": "2024-07-05T14:56:51.961448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram:\n",
      "\tCross Entropy: 8.353\n",
      "\tPerplexity: 327.069\n"
     ]
    }
   ],
   "source": [
    "sum_log_prob = 0\n",
    "bigram_cnt = 0\n",
    "alpha = 1\n",
    "\n",
    "for sent in processed_testing_sentences:\n",
    "    sent = [\"<s>\"] + sent + [\"<e>\"]\n",
    "\n",
    "    # Iterate over the bigrams of the sentence\n",
    "    for idx in range(1, len(sent)):\n",
    "        bigram_prob, bigram_log_prob = calculate_bigram_prob(\n",
    "            vocabulary, bigram_counter, unigram_counter, alpha, sent[idx - 1], sent[idx]\n",
    "        )\n",
    "\n",
    "        sum_log_prob += bigram_log_prob\n",
    "        bigram_cnt += 1\n",
    "\n",
    "HC = -sum_log_prob / bigram_cnt\n",
    "perpl = math.pow(2, HC)\n",
    "print(\"Bigram:\")\n",
    "print(\"\\tCross Entropy: {0:.3f}\".format(HC))\n",
    "print(\"\\tPerplexity: {0:.3f}\".format(perpl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74nKRvu2tL62",
    "outputId": "75c63135-6e6c-4699-b25b-abe02095415b",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:56:51.984542Z",
     "start_time": "2024-07-05T14:56:51.980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram:\n",
      "\tCross Entropy: 10.356\n",
      "\tPerplexity: 1310.347\n"
     ]
    }
   ],
   "source": [
    "sum_log_prob = 0\n",
    "trigram_cnt = 0\n",
    "alpha = 1\n",
    "\n",
    "for sent in processed_testing_sentences:\n",
    "    sent = [\"<s>\"] + [\"<s>\"] + sent + [\"<e>\"] + [\"<e>\"]\n",
    "\n",
    "    for idx in range(2, len(sent) - 1):\n",
    "        trigram_prob, trigram_log_prob = calculate_trigram_prob(\n",
    "            vocabulary,\n",
    "            trigram_counter,\n",
    "            bigram_counter,\n",
    "            alpha,\n",
    "            sent[idx - 2],\n",
    "            sent[idx - 1],\n",
    "            sent[idx],\n",
    "        )\n",
    "        sum_log_prob += trigram_log_prob\n",
    "        trigram_cnt += 1\n",
    "\n",
    "HC = -sum_log_prob / trigram_cnt\n",
    "perpl = math.pow(2, HC)\n",
    "print(\"Trigram:\")\n",
    "print(\"\\tCross Entropy: {0:.3f}\".format(HC))\n",
    "print(\"\\tPerplexity: {0:.3f}\".format(perpl))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Noisy Text Generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Bd8bEtIo5UZ8",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:56:51.987703Z",
     "start_time": "2024-07-05T14:56:51.983434Z"
    }
   },
   "outputs": [],
   "source": [
    "# method to create a new text with random errors\n",
    "def wrong_text_creator(text, error_probability=0.05):\n",
    "    result = []\n",
    "    for sentence in text:\n",
    "        wrong_sent = []\n",
    "        for char in sentence:\n",
    "            if char.isspace():\n",
    "                wrong_sent.append(char)\n",
    "            elif char.isalpha():\n",
    "                if random.random() < error_probability:\n",
    "                    wrong_sent.append(random.choice(string.ascii_letters))\n",
    "                else:\n",
    "                    wrong_sent.append(char)\n",
    "            elif char.isnumeric():\n",
    "                if random.random() < error_probability:\n",
    "                    wrong_sent.append(random.choice(string.digits))\n",
    "                else:\n",
    "                    wrong_sent.append(char)\n",
    "            elif char in string.punctuation:\n",
    "                if random.random() < error_probability:\n",
    "                    wrong_sent.append(random.choice(string.punctuation))\n",
    "                else:\n",
    "                    wrong_sent.append(char)\n",
    "            else:\n",
    "                wrong_sent.append(char)\n",
    "        result.append(\"\".join(wrong_sent))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jNaYSaITbn4o",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:56:52.072175Z",
     "start_time": "2024-07-05T14:56:51.991434Z"
    }
   },
   "outputs": [],
   "source": [
    "# Store the train bigram probabilities for beam search decoder function\n",
    "bigram_probs_dict = {}\n",
    "alpha = 1\n",
    "vocab_size = len(set(vocabulary))\n",
    "for set_of_2 in list(bigram_counter.items()):\n",
    "    bigram_prob = (set_of_2[1] + alpha) / (\n",
    "        unigram_counter[(set_of_2[0][0],)] + alpha * vocab_size\n",
    "    )\n",
    "    bigram_probs_dict[set_of_2[0]] = bigram_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Beam Search Decoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "eOIeDl7x_MRK",
    "ExecuteTime": {
     "end_time": "2024-07-05T14:56:52.075320Z",
     "start_time": "2024-07-05T14:56:52.074023Z"
    }
   },
   "outputs": [],
   "source": [
    "# Beam search decoder\n",
    "def beam_search_decoder(\n",
    "    input_sentence, bigram_probabilities, vocabulary, max_depth, beam_size=2\n",
    "):\n",
    "    input_tokens = [\"<s>\"] + word_tokenize(input_sentence) + [\"<e>\"]\n",
    "    lambda_1 = 0.5\n",
    "    lambda_2 = 0.5\n",
    "\n",
    "    beam = [([], 0)]\n",
    "    for current_word in input_tokens[1:]:\n",
    "        candidates = []\n",
    "        for candidate, candidate_score in beam:\n",
    "            for vocab_word in vocabulary:\n",
    "                temp_candidate = \"<s>\" if not candidate else candidate[-1]\n",
    "                bigram_prob = bigram_probabilities.get((temp_candidate, vocab_word), 0)\n",
    "                distance = lev.distance(current_word, vocab_word)\n",
    "                word_similarity = max(len(current_word), len(vocab_word)) - distance\n",
    "                new_score = (\n",
    "                    candidate_score\n",
    "                    + lambda_1 * math.log2(1 + bigram_prob)\n",
    "                    + lambda_2 * math.log2(1 + word_similarity)\n",
    "                )\n",
    "                new_candidate = candidate + [vocab_word]\n",
    "                candidates.append((new_candidate, new_score))\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        beam = candidates[:beam_size]\n",
    "        if len(beam[0][0]) >= max_depth:\n",
    "            break\n",
    "\n",
    "    return \" \".join(beam[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "FG6f_ZUwek5h",
    "outputId": "9199f427-01ed-41f6-f054-2aba63a702d9",
    "ExecuteTime": {
     "end_time": "2024-07-05T15:08:20.468207Z",
     "start_time": "2024-07-05T14:56:52.083182Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Correcting sentences: 100%|██████████| 619/619 [11:28<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "corrected_sentences = []\n",
    "# create the faulty text\n",
    "noisy_sentences = wrong_text_creator(testing_sentences)\n",
    "\n",
    "for wrong_sentence in tqdm(noisy_sentences, desc=\"Correcting sentences\"):\n",
    "    corrected_sentence = beam_search_decoder(\n",
    "        wrong_sentence, bigram_probs_dict, vocabulary, len(wrong_sentence)\n",
    "    )\n",
    "    corrected_sentences.append(corrected_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              Original  \\\n0    ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...   \n1    They told Reuter correspondents in Asian capit...   \n2    But some exporters said that while the conflic...   \n3    The U.S. Has said it will impose 300 mln dlrs ...   \n4    Unofficial Japanese estimates put the impact o...   \n..                                                 ...   \n614  A dividend of 11 marks would\\n  be proposed fo...   \n615  Share analysts said they saw supervisory board...   \n616  \"Anything else would be more than a surprise,\"...   \n617  Company sources said VW would have to dig into...   \n618  Parent company reserves stood at\\n  around thr...   \n\n                                                 Noisy  \\\n0    ASIAN EXPORTERS FEAR DAMAWE FROM e.S.-JAPAN RI...   \n1    They told Reuter correspondents in Asian capit...   \n2    But some exporters said thao while the Eonflic...   \n3    The U.S. Has said it will impose 340 mln dlrs ...   \n4    Unofficial Japanese estimates put the impact o...   \n..                                                 ...   \n614  A dividend of 11 marks eould\\n  be proposed fo...   \n615  Sxare analysts said they saw huperviBory board...   \n616  \"Anything else ooulj be more than a surprise,\"...   \n617  Company sources said VW wouTd have to dOg mnto...   \n618  Parent company reserves stood at\\n  aBound thr...   \n\n                                             Corrected  \n0    AUSTRALIAN EXPORTERS FEBRUARY DAMAGE FROM JAPA...  \n1    They told Reuters corresponding in Australian ...  \n2    But some exporters said ethanol while the conf...  \n3    The U.S. Has said it will impose 340 mln dlrs ...  \n4    official Japanese estimates put the impact of ...  \n..                                                 ...  \n614  A dividend of 1.1 markets previously-announced...  \n615  Share analysts said they saw superior board ap...  \n616  `` anything else obviously be more than a surp...  \n617  Company sources said Venezuela would have to d...  \n618  Parent company reserves stood at around three ...  \n\n[619 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Original</th>\n      <th>Noisy</th>\n      <th>Corrected</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...</td>\n      <td>ASIAN EXPORTERS FEAR DAMAWE FROM e.S.-JAPAN RI...</td>\n      <td>AUSTRALIAN EXPORTERS FEBRUARY DAMAGE FROM JAPA...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>They told Reuter correspondents in Asian capit...</td>\n      <td>They told Reuter correspondents in Asian capit...</td>\n      <td>They told Reuters corresponding in Australian ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>But some exporters said that while the conflic...</td>\n      <td>But some exporters said thao while the Eonflic...</td>\n      <td>But some exporters said ethanol while the conf...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The U.S. Has said it will impose 300 mln dlrs ...</td>\n      <td>The U.S. Has said it will impose 340 mln dlrs ...</td>\n      <td>The U.S. Has said it will impose 340 mln dlrs ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Unofficial Japanese estimates put the impact o...</td>\n      <td>Unofficial Japanese estimates put the impact o...</td>\n      <td>official Japanese estimates put the impact of ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>614</th>\n      <td>A dividend of 11 marks would\\n  be proposed fo...</td>\n      <td>A dividend of 11 marks eould\\n  be proposed fo...</td>\n      <td>A dividend of 1.1 markets previously-announced...</td>\n    </tr>\n    <tr>\n      <th>615</th>\n      <td>Share analysts said they saw supervisory board...</td>\n      <td>Sxare analysts said they saw huperviBory board...</td>\n      <td>Share analysts said they saw superior board ap...</td>\n    </tr>\n    <tr>\n      <th>616</th>\n      <td>\"Anything else would be more than a surprise,\"...</td>\n      <td>\"Anything else ooulj be more than a surprise,\"...</td>\n      <td>`` anything else obviously be more than a surp...</td>\n    </tr>\n    <tr>\n      <th>617</th>\n      <td>Company sources said VW would have to dig into...</td>\n      <td>Company sources said VW wouTd have to dOg mnto...</td>\n      <td>Company sources said Venezuela would have to d...</td>\n    </tr>\n    <tr>\n      <th>618</th>\n      <td>Parent company reserves stood at\\n  around thr...</td>\n      <td>Parent company reserves stood at\\n  aBound thr...</td>\n      <td>Parent company reserves stood at around three ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>619 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the original, noisy and corrected sentences into a dataframe so that we can compare them\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Original\": testing_sentences,\n",
    "        \"Noisy\": noisy_sentences,\n",
    "        \"Corrected\": corrected_sentences,\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-05T15:08:20.475919Z",
     "start_time": "2024-07-05T15:08:20.470516Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Ms2GTfxFA1bg",
    "outputId": "b68c11c0-8c75-40ad-cc90-513cb7d9f0a8",
    "ExecuteTime": {
     "end_time": "2024-07-05T15:09:15.069374Z",
     "start_time": "2024-07-05T15:09:13.844562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER score: 0.493\n",
      "CER score: 0.273\n"
     ]
    }
   ],
   "source": [
    "# Character error rate metric\n",
    "cer = load_metric(\"cer\")\n",
    "# Word error rate metric\n",
    "wer = load_metric(\"wer\")\n",
    "\n",
    "truth_sentences = testing_sentences\n",
    "\n",
    "# Compute the WER and CER scores\n",
    "wer_score = wer.compute(predictions=corrected_sentences, references=truth_sentences)\n",
    "cer_score = cer.compute(predictions=corrected_sentences, references=truth_sentences)\n",
    "\n",
    "print(\"WER score: {0:.3f}\".format(wer_score))\n",
    "print(\"CER score: {0:.3f}\".format(cer_score))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
